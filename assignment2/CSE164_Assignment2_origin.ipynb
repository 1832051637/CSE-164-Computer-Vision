{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF8KpEYHcxJK"
      },
      "source": [
        "\n",
        "<h1 align=\"center\"> CSE 164 Assignment 2, Spring 2022\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgJO1baFcxJQ"
      },
      "source": [
        "\n",
        "<h2 align=\"center\"> 8 Questions, 80 pts, due: 23:59 pm, May 8th, 2022\n",
        "    \n",
        "    Your name: Erjie Zhang         Student ID: 1813132 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbDda_HHcxJR"
      },
      "source": [
        "## Instruction \n",
        "\n",
        "- In this assignment, you will learn how to extract SIFT discriptors (partially) and construct a panorama by stitching several individual and overlapping images together.\n",
        "    - **Problem 1: Difference-of-Gaussian (10 points)**\n",
        "        - Implement the `generateDoGImages` function.\n",
        "    - **Problem 2: Finding Extremum (10 points)**\n",
        "        - Implement the `isPixelAnExtremum` function.\n",
        "    - **Problem 3: Histogram Counting (10 points)**\n",
        "        - Implement the `computeKeypointsWithOrientations` function.\n",
        "    - **Problem 4: Homography (10 points)**\n",
        "        - Implement the `compute_homography` function.\n",
        "    - **Problem 5: Warping (10 points)**\n",
        "        - Implement the `backward_warp_img` function.\n",
        "    - **Problem 6: SIFT and RANSAC (10 points)**\n",
        "        - Implement the `RANSAC` function.\n",
        "    - **Problem 7: Image Blending (10 points)**\n",
        "        - Implement the `blend_image_pair` function.\n",
        "    - **Problem 8: Creating Panoramas (10 points)**\n",
        "        - Implement the `stitch_img` function.\n",
        "\n",
        "- Your job is to implement the sections marked with TODO to complete the tasks.\n",
        "\n",
        "- Submit your assignments onto **Canvas** by the due date. Upload a <code>zip</code> file containing:\n",
        "\n",
        "    (1) The saved/latest <code>.ipynb</code> file including the output of all cells.\n",
        "    \n",
        "    (2) All other materials to make your <code>.ipynb</code> file runnable.\n",
        "    \n",
        "## Note\n",
        "- This is an **individual** assignment. All help from others (from the web, books other than text, or people other than the TA or instructor) must be clearly acknowledged. \n",
        "- Don't use any magic function from other libraries. You will get **no credit** if a Gaussian function from a known library is used when you are asked to implement “Gaussian Filtering” from scratch.\n",
        "- Don't change the input and output structure of pre-defined functions.  Most coding parts can be finished with less than 5 lines of codes.\n",
        "- Make sure you have installed required packages: <code>numpy</code>, <code>matplotlib</code>, <code>PIL</code>, <code>opencv</code>.\n",
        "\n",
        "## Objective \n",
        "\n",
        "- **Task 1:** Scale Invariant Feature Transform Image Descriptor\n",
        "- **Task 2:** Image Stitching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFHenRoycxJS"
      },
      "source": [
        "Load the Dependencies\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MnOvoqacxJS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "import sys\n",
        "import random\n",
        "import cv2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYMtmMXdcxJU"
      },
      "source": [
        "SIFT Main Function\n",
        "-----\n",
        "\n",
        "Here is the main logic of extracting SIFT keypoints and descriptors. As SIFT is far complicated for you to master every details of it (~500 lines of code in total), and involves some advanced techniques that are not covered in the lecture, here you are only asked to implement some parts of SIFT. Nevertheless, this (function naming, comment, etc) should be enough to give you a rough idea about how SIFT works.\n",
        "\n",
        "Don't panic when you see some undefined functions here. **For the first three problems related to SIFT, you don't need to actually run the code. Just fill in the blanks.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJLbE342cxJV"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "# Main function #\n",
        "#################\n",
        "\n",
        "def computeKeypointsAndDescriptors(image, sigma=1.6, num_intervals=3, assumed_blur=0.5, image_border_width=5):\n",
        "    \"\"\"Compute SIFT keypoints and descriptors for an input image\n",
        "    \"\"\"\n",
        "    image = image.astype('float32')\n",
        "    base_image = generateBaseImage(image, sigma, assumed_blur)\n",
        "    num_octaves = computeNumberOfOctaves(base_image.shape)\n",
        "    gaussian_kernels = generateGaussianKernels(sigma, num_intervals)\n",
        "    gaussian_images = generateGaussianImages(base_image, num_octaves, gaussian_kernels)\n",
        "    dog_images = generateDoGImages(gaussian_images)\n",
        "    keypoints = findScaleSpaceExtrema(gaussian_images, dog_images, num_intervals, sigma, image_border_width)\n",
        "    keypoints = removeDuplicateKeypoints(keypoints)\n",
        "    keypoints = convertKeypointsToInputImageSize(keypoints)\n",
        "    descriptors = generateDescriptors(keypoints, gaussian_images)\n",
        "    return keypoints, descriptors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKH8O_CocxJW"
      },
      "source": [
        "Problem 1: Difference-of-Gaussian (10 points)\n",
        "-----\n",
        "\n",
        "**generateGaussianImages** starts with our base image and successively blurs it according to the gaussian_kernels. Note that we skip the first element of gaussian_kernels because we begin with an image that already has that blur value. We halve the third-to-last image, since this has the appropriate blur we want, and we use it as the base image for the next layer.\n",
        "\n",
        "**generateDoGImages** subtracts successive pairs of these Gaussian-blurred images as an approximation of the Normalized-Laplacian-of-Gaussian (NLoG) of these images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqIFZBvicxJX"
      },
      "outputs": [],
      "source": [
        "#########################\n",
        "# Image pyramid related #\n",
        "#########################\n",
        "def generateGaussianImages(image, num_octaves, gaussian_kernels):\n",
        "    \"\"\"Generate scale-space pyramid of Gaussian images\n",
        "\n",
        "    Input:\n",
        "        image: base imgae to be blurred, shape (m, n). we assume grayscale image here.\n",
        "        num_octaves: number of gaussian images octaves. int.\n",
        "        gaussian_kernels: standard deviation of pre-computed gaussian_kernels for blurring the base image, each element is the multiplication of a constant and the previous element except for the first element. shape (k, )\n",
        "    Output:\n",
        "        gaussian_images: each element of it is a gaussian images octave, which contains the differently gaussian blurred version of the same base image.\n",
        "        Note that the shape is halved in consecutive gaussian images octaves.\n",
        "    \"\"\"\n",
        "    gaussian_images = []\n",
        "\n",
        "    for octave_index in range(num_octaves):\n",
        "        gaussian_images_in_octave = []\n",
        "        gaussian_images_in_octave.append(image)  # first image in octave already has the correct blur\n",
        "        for gaussian_kernel in gaussian_kernels[1:]:\n",
        "            image = cv2.GaussianBlur(image, (0, 0), sigmaX=gaussian_kernel, sigmaY=gaussian_kernel)\n",
        "            gaussian_images_in_octave.append(image)\n",
        "        gaussian_images.append(gaussian_images_in_octave)\n",
        "        octave_base = gaussian_images_in_octave[-3]\n",
        "        image = cv2.resize(octave_base, (int(octave_base.shape[1] / 2), int(octave_base.shape[0] / 2)), interpolation=cv2.INTER_NEAREST)\n",
        "    return np.array(gaussian_images, dtype=object) # numpy array with dtype=object can be seen as a special kind of list in python. See https://numpy.org/doc/stable/reference/arrays.dtypes.html for details.\n",
        "\n",
        "def generateDoGImages(gaussian_images):\n",
        "    \"\"\"Generate Difference-of-Gaussians image pyramid\n",
        "\n",
        "    Input:\n",
        "        gaussian_images: each element of it is a gaussian images octave, which contains the differently gaussian blurred version of the same base image.\n",
        "        Note that the shape is halved in consecutive gaussian images octaves.\n",
        "    Output:\n",
        "        dog_images: each element of it is a dog images octave, which contains the dog image created with different standard deviations from the same base image.\n",
        "        Note that the shape is halved in consecutive dog images octaves.\n",
        "\n",
        "    TODO: append dog image to dog_images_in_octave\n",
        "    \"\"\"\n",
        "    dog_images = []\n",
        "\n",
        "    for gaussian_images_in_octave in gaussian_images:\n",
        "        dog_images_in_octave = []\n",
        "        for first_image, second_image in zip(gaussian_images_in_octave, gaussian_images_in_octave[1:]):\n",
        "            ################## Your code begins here #######################\n",
        "            dog_images_in_octave.append()    \n",
        "            ################# Your code ends here ###########################\n",
        "        dog_images.append(dog_images_in_octave)\n",
        "    return np.array(dog_images, dtype=object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMx0o_tAcxJX"
      },
      "source": [
        "Problem 2: Finding Scale Space Extrema (10 points)\n",
        "-----\n",
        "\n",
        "In each triplet of images, we look for pixels in the middle image that are greater than or less than all of their 26 neighbors: 8 neighbors in the middle image, 9 neighbors in the image below, and 9 neighbors in the image above. The function ****isPixelAnExtremum** check if the center pixel in the middle image is an extremum. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaJdAYKtcxJY"
      },
      "outputs": [],
      "source": [
        "def isPixelAnExtremum(first_subimage, second_subimage, third_subimage, threshold):\n",
        "    \"\"\"Return True if the center element of the 3x3x3 input array is strictly greater than or less than all its neighbors, False otherwise\n",
        "\n",
        "    Input:\n",
        "        first_subimage: image patch of shape (3, 3)\n",
        "        second_subimage: image patch of shape (3, 3)\n",
        "        third_subimage: image patch of shape (3, 3)\n",
        "        threshold: used to filter out those patches with too low reponses. int.\n",
        "    Output:\n",
        "        True if the center element is a extremum, False otherwise\n",
        "\n",
        "    TODO: implement the isPixelAnExtremum function.\n",
        "    \"\"\"\n",
        "    center_pixel_value = second_subimage[1, 1]\n",
        "    if abs(center_pixel_value) > threshold:\n",
        "        ################## Your code begins here #######################\n",
        "        return \n",
        "        ################## Your code ends here #######################\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giul7LprcxJY"
      },
      "source": [
        "Problem 3: Histogram Counting for Keypoints Orientations (10 points)\n",
        "-----\n",
        "\n",
        "The goal of **computeKeypointsWithOrientations** is to create a histogram of gradients for pixels around the keypoint’s neighborhood. \n",
        "\n",
        "A 36-bin histogram for the orientations (10 degrees per bin) is created. The orientation of a particular pixel decides which histogram bin to choose, but the actual value to be assigned in that bin is that pixel’s gradient magnitude with a Gaussian weighting. This makes pixels farther from the keypoint have less of an influence on the histogram. \n",
        "\n",
        "Then the program looks for peaks in this histogram that lie above a threshold specified in the SIFT paper, using quadratic interpolation. (which is again an adanvced technique that is not covered and not required in this course. Check the [original paper](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf) for details.) \n",
        "\n",
        "One thing to notice is that a separate keypoint for each peak is created, and these keypoints will be identical except for their orientation attributes. As stated in the SIFT paper, these additional keypoints significantly contribute to detection stability when used in real applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1u0EsGcxJZ"
      },
      "outputs": [],
      "source": [
        "def computeKeypointsWithOrientations(keypoint, octave_index, gaussian_image, radius_factor=3, num_bins=36, peak_ratio=0.8, scale_factor=1.5):\n",
        "    \"\"\"Compute orientations for each keypoint\n",
        "\n",
        "    Input:\n",
        "        keypoint: opencv keypoint. check https://docs.opencv.org/3.4/d2/d29/classcv_1_1KeyPoint.html#a507d41b54805e9ee5042b922e68e4372 for details\n",
        "        octave_index: which octave the detected keypoint corresponds to. int\n",
        "        gaussian_image: the gaussian blurred image corresponding to the detected keypoint\n",
        "        radius_factor: controls the size of region for computing histogram. int\n",
        "        num_bins: number of bins in the histogram. int.\n",
        "        peak_ratio: decides how mant separate keypoins will be created other than the one with max peak. float.\n",
        "        scale_factor: controls how much the keypoint size is enlarged in the calculation. float.\n",
        "    Output:\n",
        "        a list of keypoints, each one of them has a new orientation attribute\n",
        "\n",
        "    TODO: compute histogram index and assign weighted gradient magnitude to the corresponding histogram entry\n",
        "    \n",
        "    \"\"\"\n",
        "    image_shape = gaussian_image.shape\n",
        "\n",
        "    scale = scale_factor * keypoint.size / np.float32(2 ** (octave_index + 1))  # compare with keypoint.size computation in localizeExtremumViaQuadraticFit()\n",
        "    radius = int(round(radius_factor * scale))\n",
        "    weight_factor = -0.5 / (scale ** 2)\n",
        "    raw_histogram = np.zeros(num_bins)\n",
        "    \n",
        "\n",
        "    for i in range(-radius, radius + 1):\n",
        "        region_y = int(round(keypoint.pt[1] / np.float32(2 ** octave_index))) + i\n",
        "        if region_y > 0 and region_y < image_shape[0] - 1:\n",
        "            for j in range(-radius, radius + 1):\n",
        "                region_x = int(round(keypoint.pt[0] / np.float32(2 ** octave_index))) + j\n",
        "                if region_x > 0 and region_x < image_shape[1] - 1:\n",
        "                    dx = gaussian_image[region_y, region_x + 1] - gaussian_image[region_y, region_x - 1]\n",
        "                    dy = gaussian_image[region_y - 1, region_x] - gaussian_image[region_y + 1, region_x]\n",
        "                    gradient_magnitude = np.sqrt(dx * dx + dy * dy)\n",
        "                    gradient_orientation = np.rad2deg(np.arctan2(dy, dx))\n",
        "                    weight = np.exp(weight_factor * (i ** 2 + j ** 2))  # constant in front of exponential can be dropped because we will find peaks later\n",
        "                    # note that the gradient orintation is in degrees\n",
        "                    ################## Your code begins here #######################\n",
        "                    histogram_index = \n",
        "                    raw_histogram[histogram_index] += \n",
        "                    ################# Your code ends here ###########################\n",
        "                    \n",
        "\n",
        "    keypoints_with_orientations = postProcessHistogram(num_bins, raw_histogram, peak_ratio, keypoint) # localize each peak using quadratic interpolation, optional advanced technique\n",
        "    return keypoints_with_orientations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj6phicQcxJZ"
      },
      "source": [
        "Image Stitching Setup\n",
        "-----\n",
        "\n",
        "Before we start image stiching, let's visualize the three separate images we ultimately want to stitch together. \n",
        "\n",
        "**From now on, you are required to run every cell and your submission should contain the output.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1WGcFTascxJa"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [15, 15]\n",
        "\n",
        "def load_image(filename):\n",
        "    img = np.asarray(Image.open(filename))\n",
        "    img = img.astype(\"float32\")/255.\n",
        "    return img\n",
        "\n",
        "def show_image(img):\n",
        "    plt.imshow(img, interpolation='nearest')\n",
        "    \n",
        "center_img = load_image(\"mountain_center.png\")\n",
        "left_img   = load_image(\"mountain_left.png\")\n",
        "right_img  = load_image(\"mountain_right.png\")\n",
        "\n",
        "show_image(np.concatenate([left_img, center_img, right_img], axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unkXZLTBcxJa"
      },
      "source": [
        "Problem 4: Homography (10 points)\n",
        "=========\n",
        "\n",
        "You should finish the **compute_homography(src, dst)** function. It receives two matrices of points, which are each Nx2. The function should return the homography matrix H that maps points from the source to the target. This return value should be a 3x3 matrix. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te6l1mLGcxJb"
      },
      "outputs": [],
      "source": [
        "def compute_homography(src, dst):\n",
        "    '''Computes the homography from src to dst.\n",
        "    \n",
        "    Input:\n",
        "        src: source points, shape (n, 2)\n",
        "        dst: destination points, shape (n, 2)\n",
        "    Output:\n",
        "        H: homography from source points to destination points, shape (3, 3)\n",
        "        \n",
        "    TODO: Implement the A matrix. \n",
        "    '''\n",
        "    \n",
        "    A = np.zeros([2*src.shape[0], 9])\n",
        "    # Your code here.\n",
        "    ################## Your code begins here #####################\n",
        "    for i in range(src.shape[0]):\n",
        "        A[2*i,:] = \n",
        "        A[2*i+1,:] = \n",
        "    ################## Your code ends here #####################\n",
        "    \n",
        "    w, v = np.linalg.eig(np.dot(A.T, A))\n",
        "    index = np.argmin(w)\n",
        "    H = v[:, index].reshape([3,3])\n",
        "    return H\n",
        "\n",
        "def apply_homography(src, H):\n",
        "    '''Applies a homography H onto the source points.\n",
        "    \n",
        "    Input:\n",
        "        src: source points, shape (n, 2)\n",
        "        H: homography from source points to destination points, shape (3, 3)\n",
        "    Output:\n",
        "        dst: destination points, shape (n, 2)\n",
        "    '''\n",
        "    ones = np.ones((src.shape[0],1))\n",
        "    src = np.hstack([src,ones])\n",
        "    dst = np.dot(H,src.T)\n",
        "    dst = dst / dst[2]\n",
        "    \n",
        "    return dst[:2,:].T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW57yTd3cxJc"
      },
      "source": [
        "To help you debug the homography code, we have provided a test below. This uses pairs of points (src_pts and dst_pts) to compute the homography. Then, it applies the homography on held-out points (test_pts), and visualizes the correspondence as red lines between the two images. If you have correctly implemented compute_homography() and apply_homography, the red lines should connect the same points in both images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrjRAXQacxJc"
      },
      "outputs": [],
      "source": [
        "def test_homography():\n",
        "    src_img = load_image('portrait.png')[:, :, :3]\n",
        "    dst_img = load_image('portrait_transformed.png')\n",
        "    whole_img = np.concatenate((src_img, dst_img), axis=1)\n",
        "\n",
        "    src_pts = np.matrix('347, 313; 502, 341; 386, 571; 621, 508')\n",
        "    dst_pts = np.matrix('274, 286; 436, 305; 305, 527; 615, 506')\n",
        "    H = compute_homography(src_pts, dst_pts)\n",
        "\n",
        "    test_pts = np.matrix('259, 505; 350, 371; 400, 675; 636, 104')\n",
        "    match_pts = apply_homography(test_pts, H)\n",
        "\n",
        "    match_pts_correct = np.matrix('195.13761083, 448.12645033;'\n",
        "                                  '275.27269386, 336.54819916;'\n",
        "                                  '317.37663747, 636.78403426;'\n",
        "                                  '618.50438823, 28.78963905')\n",
        "\n",
        "    print('Your solution differs from our solution by: %f'\n",
        "          % np.square(match_pts - match_pts_correct).sum())\n",
        "\n",
        "    for i in range(test_pts.shape[0]):\n",
        "        test_x = test_pts[i, 0]\n",
        "        test_y = test_pts[i, 1]\n",
        "        match_x = int(round(match_pts[i, 0] + 800))\n",
        "        match_y = int(round(match_pts[i, 1]))\n",
        "\n",
        "        cv2.line(whole_img,\n",
        "            (test_x, test_y), \n",
        "            (match_x, match_y), \n",
        "            (255, 0, 0), thickness=5)\n",
        "        cv2.circle(whole_img,\n",
        "            (test_x, test_y),\n",
        "            4, (255, 0, 0), thickness=10)\n",
        "        cv2.circle(whole_img,\n",
        "            (match_x, match_y),\n",
        "            4, (255, 0, 0), thickness=10)\n",
        "\n",
        "    print('If your solution is correct, the red lines will match to the same points in both images below:')\n",
        "    show_image(np.clip(whole_img, 0, 1)) \n",
        "\n",
        "test_homography()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOEOXL4YcxJd"
      },
      "source": [
        "Problem 5: Warping (10 points)\n",
        "=====\n",
        "\n",
        "When we map a source image to its destination image using a homography, we may encounter a problem where multiple pixels of the source image are mapped to the same point of its destination image. What's more, some pixels of the destination image may not be mapped to any pixels of source image. What should we do?\n",
        "\n",
        "Suppose we had homography $H$, source pixel $s$ with coordinates $(x_s, y_s)$, and destination pixel $d$ with coordinates $(x_d, y_d)$. Then, $H \\cdot \\tilde{s} = \\tilde{d}$ (where, $s$, $d$ are in homogenous space).\n",
        "\n",
        "To deal with this problem, we warp in the opposite direction: we map the pixels of the destination image back to source image, and then use the color in the source image as its color. More precisely, for each destination pixel $d = (x_d, y_d)$, we take $H^{-1} \\cdot \\tilde{d}$ to obtain the coordinate of its associated source pixel, $\\tilde{s}$ (from which $s$ can be found). If $s$ is within the bounds of the source image, we take the intensity of $s$ to be the intensity of $d$.\n",
        "\n",
        "Repeating this process over the entire destination image ensures that there are no gaps in the final result. This process is called \"backward warping\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYvIsk7dcxJd"
      },
      "outputs": [],
      "source": [
        "def backward_warp_img(src_img, H, dst_img_size):\n",
        "    '''Backward warping of the source image using a homography.\n",
        "    \n",
        "    Input:\n",
        "        src_img: source image, shape (m, n, 3)\n",
        "        H: homography from destination to source image, shape (3, 3)\n",
        "        dst_img_size: height and width of destination image, shape (2,)\n",
        "    Output:\n",
        "        dst_img: destination image, shape (m, n, 3)\n",
        "    \n",
        "    TODO: Implement the backward_warp_img function. \n",
        "    '''\n",
        "    dict_matrix = np.ones((3,dst_img_size[0]*dst_img_size[1]))\n",
        "    dict_matrix[1,:] = np.array([\n",
        "        [i] * dst_img_size[1] for i in range(dst_img_size[0])\n",
        "    ]).flatten()\n",
        "    dict_matrix[0,:] = np.array([\n",
        "        [i for i in range(dst_img_size[1])] * dst_img_size[0]\n",
        "    ]).flatten()\n",
        "\n",
        "    dict_matrix = np.dot(H,dict_matrix)\n",
        "    dict_matrix = dict_matrix / dict_matrix[2,:]\n",
        "    dict_matrix = dict_matrix.astype(np.int32)\n",
        "    \n",
        "    dst_img = np.zeros((dst_img_size[0], dst_img_size[1], 3))\n",
        "\n",
        "    for r in range(dst_img_size[0]):\n",
        "        for c in range(dst_img_size[1]):\n",
        "            ################## Your code begins here #####################\n",
        "            x = \n",
        "            y = \n",
        "            ################## Your code ends here #######################\n",
        "            if x >=0 and y >=0 and x < src_img.shape[1] and y < src_img.shape[0]:\n",
        "                dst_img[r,c,:] = src_img[y, x, :]\n",
        "    return dst_img\n",
        "\n",
        "def binary_mask(img):\n",
        "    '''Create a binary mask of the image content.\n",
        "    \n",
        "    Input:\n",
        "        img: source image, shape (m, n, 3)\n",
        "    Output:\n",
        "        mask: image of shape (m, n) and type 'int'. For pixel [i, j] of mask, if img[i, j] > 0 \n",
        "              in any of its channels, mask[i, j] = 1. Else, (if img[i, j] = 0), mask[i, j] = 0.\n",
        "    '''\n",
        "\n",
        "    mask = (img[:, :, 0] > 0) | (img[:, :, 1] > 0) | (img[:, :, 2] > 0)\n",
        "    mask = mask.astype(\"int\")\n",
        "    \n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qef59l7JcxJd"
      },
      "source": [
        "Use the function below to help debug your implementation. If it is correct, it should warp Van Gogh's self-portrait onto the building side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd0ru9J0cxJd"
      },
      "outputs": [],
      "source": [
        "def test_warp():\n",
        "    src_img = load_image('portrait_small.png')\n",
        "    canvas = load_image('Osaka.png')\n",
        "\n",
        "    src_pts = np.matrix('1, 1; 1, 400; 326, 1; 326, 400')\n",
        "    canvas_pts = np.matrix('100, 18; 84, 437; 276, 71; 286, 424')\n",
        "    H = compute_homography(src_pts, canvas_pts)\n",
        "\n",
        "    dst_img = backward_warp_img(src_img, np.linalg.inv(H), [canvas.shape[0], canvas.shape[1]])\n",
        "    dst_mask = 1 - binary_mask(dst_img)\n",
        "    dst_mask = np.stack((dst_mask,) * 3, -1)\n",
        "    out_img = np.multiply(canvas, dst_mask) + dst_img\n",
        "\n",
        "    warp_img = np.concatenate((canvas, out_img), axis=1)\n",
        "\n",
        "    show_image(np.clip(warp_img, 0, 1))\n",
        "    \n",
        "test_warp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4vhveIkcxJe"
      },
      "source": [
        "Problem 6: SIFT and RANSAC (10 points)\n",
        "====\n",
        "\n",
        "SIFT Keypoints\n",
        "--------------\n",
        "\n",
        "So far, we have manually defined corresponding keypoints for both estimating homographies and warping. We want to automate this now. However, if we just take two photos, how do we know which points correspond? We could estimate SIFT keypoints, and take the nearest neighbor between them. The code below computes SIFT keypoints, and visualizes the matches. Note that you don't have to do anything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ5t2vVKcxJe"
      },
      "outputs": [],
      "source": [
        "def genSIFTMatchPairs(img1, img2):\n",
        "    sift = cv2.SIFT_create() # if you encounter error in calling this function in opencv, try installing opencv-contrib-python and calling cv2.xfeatures2d.SIFT_create instead\n",
        "    # sift = cv2.xfeatures2d.SIFT_create()\n",
        "    kp1, des1 = sift.detectAndCompute(img1, None)\n",
        "    kp2, des2 = sift.detectAndCompute(img2, None)\n",
        "\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
        "    matches = bf.match(des1, des2)\n",
        "    matches = sorted(matches, key = lambda x:x.distance)\n",
        "    \n",
        "    pts1 = np.zeros((250,2))\n",
        "    pts2 = np.zeros((250,2))\n",
        "    for i in range(250):\n",
        "        pts1[i,:] = kp1[matches[i].queryIdx].pt\n",
        "        pts2[i,:] = kp2[matches[i].trainIdx].pt\n",
        "    \n",
        "    return pts1, pts2, matches[:250], kp1, kp2\n",
        "\n",
        "def test_matches():\n",
        "    img1 = cv2.imread('mountain_left.png')\n",
        "    img2 = cv2.imread('mountain_center.png')\n",
        "\n",
        "    pts1, pts2, matches, kp1, kp2 = genSIFTMatchPairs(img1, img2)\n",
        "\n",
        "    matching_result = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags=2, matchColor=(0,0,255))\n",
        "    plt.imshow(cv2.cvtColor(matching_result, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "test_matches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKNLXr-VcxJe"
      },
      "source": [
        "Notice that the matches are not all correct. There is a substantial amount of noise or incorrect matches. If we include these wrong matches in our homography estimation, what will happen? Think about this, and convince yourself why it will not work well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgPiDGppcxJe"
      },
      "source": [
        "RANSAC\n",
        "------\n",
        "Instead, we will use RANSAC, which is an optimization algorithm that finds correspondences while also discarding the outliers. Implement the **RANSAC** function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmxQMLOUcxJf"
      },
      "outputs": [],
      "source": [
        "def RANSAC(Xs, Xd, max_iter, eps):\n",
        "    '''Finds correspondences between two sets of points using the RANSAC algorithm.\n",
        "    \n",
        "    Input:\n",
        "        Xs: the first set of points (source), shape [n, 2]\n",
        "        Xd: the second set of points (destination) matched to the first set, shape [n, 2]\n",
        "        max_iter: max iteration number of RANSAC\n",
        "        eps: tolerance of RANSAC\n",
        "    Output:\n",
        "        inliers_id: the indices of matched pairs when using the homography given by RANSAC\n",
        "        H: the homography, shape [3, 3]\n",
        "    \n",
        "    TODO: Implement the RANSAC function. \n",
        "    '''\n",
        "    \n",
        "    inliers_id = []\n",
        "    best_count = 0\n",
        "#     best_H = None\n",
        "    for _ in range(max_iter):\n",
        "        samples = np.random.choice(Xs.shape[0], 4)\n",
        "        H = compute_homography(Xs[samples,:], Xd[samples,:])\n",
        "        dst = apply_homography(Xs, H)\n",
        "        distance = np.linalg.norm(dst-Xd,axis=1)\n",
        "        \n",
        "        correct_num = 0\n",
        "        samples = []\n",
        "        for index, diff in enumerate(distance):\n",
        "            if diff < eps:\n",
        "                correct_num += 1\n",
        "                samples.append(index)\n",
        "                \n",
        "        if correct_num > best_count:\n",
        "            ################## Your code begins here #####################\n",
        "\n",
        "            ################## Your code begins here #####################\n",
        "    \n",
        "    ################## Your code begins here #####################\n",
        "    H = \n",
        "    ################## Your code begins here #####################\n",
        "\n",
        "    return inliers_id, H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUvth2HxcxJf"
      },
      "source": [
        "Now, let's visualize the matches between keypoints after using your RANSAC implementation. If you implemented RANSAC correctly, the outlier matches should be automatically discarded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "qDhwcgCdcxJf"
      },
      "outputs": [],
      "source": [
        "def test_ransac():\n",
        "    img1 = cv2.imread('mountain_left.png')\n",
        "    img2 = cv2.imread('mountain_center.png')\n",
        "\n",
        "    pts1, pts2, matches, kp1, kp2 = genSIFTMatchPairs(img1, img2)\n",
        "    \n",
        "    inliers_idx, H = RANSAC(pts1, pts2, 500, 20)\n",
        "\n",
        "    new_matches = []\n",
        "    for i in range(len(inliers_idx)):\n",
        "        new_matches.append(matches[inliers_idx[i]])\n",
        "\n",
        "    matching_result = cv2.drawMatches(img1, kp1, img2, kp2, new_matches, None, flags=2, matchColor=(0,0,255))\n",
        "    plt.imshow(cv2.cvtColor(matching_result, cv2.COLOR_BGR2RGB))\n",
        "    \n",
        "test_ransac()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mykb4DnacxJf"
      },
      "source": [
        "Problem 7: Image Blending (10 points)\n",
        "====\n",
        "\n",
        "We have now implemented code to estimate correspondences between photos, estimate the homography, and warp one image into the other image. Before we can build our panorama making application, the next piece we need is code to seamlessly blend two images together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i_s-rKncxJf"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage.morphology import distance_transform_edt as euc_dist\n",
        "\n",
        "def blend_image_pair(src_img, src_mask, dst_img, dst_mask, mode):\n",
        "    '''Given two images and their binary masks, blend the two images.\n",
        "    \n",
        "    Input:\n",
        "        src_img: First image to be blended, shape (m, n, 3)\n",
        "        src_mask: src_img's binary mask, shape (m, n)\n",
        "        dst_img: Second image to be blended, shape (m, n, 3)\n",
        "        dst_mask: dst_img's binary mask, shape (m, n)\n",
        "        mode: Blending mode, either \"overlay\" or \"blend\"\n",
        "    Output:\n",
        "        Blended image of shape (m, n, 3)\n",
        "    \n",
        "    TODO: Implement the blend_image_pair function.\n",
        "    '''\n",
        "    if mode == 'blend':\n",
        "        blend_img = np.zeros_like(src_img)\n",
        "        w1  = euc_dist(src_mask) # computes the distance from non-zero (i.e. non-background) points to the nearest zero (i.e. background) point.\n",
        "        w2  = euc_dist(dst_mask)\n",
        "\n",
        "        w1 = np.expand_dims(w1,axis=-1)\n",
        "        w2 = np.expand_dims(w2,axis=-1)\n",
        "\n",
        "        ################## Your code begins here #####################\n",
        "        weight_sum = \n",
        "        weight_img = \n",
        "        ################## Your code ends here #####################\n",
        "        flag = (weight_sum == 0).astype(float)\n",
        "        weight_sum += flag\n",
        "\n",
        "        blend_img = weight_img / weight_sum\n",
        "        \n",
        "    else:\n",
        "        dst_mask = np.tile(np.expand_dims(dst_mask,axis=-1),(1,1,3))\n",
        "        blend_img = src_img - src_img*dst_mask + dst_img\n",
        "        \n",
        "    return blend_img.astype(src_img.dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DA8p07scxJg"
      },
      "source": [
        "To test your implementation, you can use the function below. It supports two modes. Setting mode=\"blend\" should seamlessly blend the two images. Setting mode=\"overlay\" will just combine them without any blending."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiwr8c1bcxJg"
      },
      "outputs": [],
      "source": [
        "def test_blend(mode):\n",
        "    fish_img = load_image(\"escher_fish.png\")[:, :, :3]\n",
        "    horse_img = load_image(\"escher_horsemen.png\")[:, :, :3]\n",
        "\n",
        "    blend_img = blend_image_pair(fish_img, binary_mask(fish_img), horse_img, binary_mask(horse_img), mode)\n",
        "\n",
        "    f, axarr = plt.subplots(1,3)\n",
        "    axarr[0].imshow(fish_img, cmap='gray')\n",
        "    axarr[1].imshow(horse_img, cmap='gray')\n",
        "    axarr[2].imshow(blend_img, cmap='gray')\n",
        "    \n",
        "test_blend(\"blend\")\n",
        "test_blend(\"overlay\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOy83m1WcxJg"
      },
      "source": [
        "Problem 8: Creating Panoramas (10 points)\n",
        "====\n",
        "\n",
        "We are now ready to make a panorama from the three images at the beginning. The function below receives a Python list of images, which you should stitch together to form one large image. You will need to call most of the functions defined above in order to successfully do this. \n",
        "\n",
        "To receive full credit, make sure you have stitched the three images together with very little seam between them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wE8XnDQcxJg"
      },
      "outputs": [],
      "source": [
        "def stitch_img(imgs):\n",
        "    '''Stitch a list of images together.\n",
        "    \n",
        "    Input: \n",
        "        imgs: a list of images.\n",
        "    Output:\n",
        "        stitched_img: a single stiched image.\n",
        "        \n",
        "    TODO: implement the stitch_img function. \n",
        "    '''\n",
        "    num_imgs = len(imgs)\n",
        "    if num_imgs < 2:\n",
        "        raise ValueError('insufficient images!')\n",
        "        \n",
        "    base_img = imgs[0]\n",
        "    canvas = np.zeros_like(base_img)\n",
        "    canvas = np.tile(canvas,(1,num_imgs - 1,1))\n",
        "    panorama = np.hstack([canvas,base_img,canvas])\n",
        "\n",
        "    left_img = panorama\n",
        "    right_img = panorama\n",
        "    \n",
        "    unused_imgs = imgs[1:]\n",
        "    \n",
        "    trial = 0\n",
        "    \n",
        "    while len(unused_imgs) != 0 and trial < 2 * num_imgs:\n",
        "        trial += 1\n",
        "        print('Stitch {} images'.format(trial))\n",
        "        current_img = unused_imgs.pop(0)\n",
        "    \n",
        "        cur_pts, pano_pts,_,_,_ = genSIFTMatchPairs(current_img, panorama)\n",
        "    \n",
        "        if len(cur_pts) < 4:\n",
        "            unused_imgs.append(current_img)\n",
        "            print('Current image cannot be stitched. Would try later...')\n",
        "            continue\n",
        "\n",
        "        ################## Your code begins here #####################\n",
        "        # use max_iter=5000 and eps=15 for RANSAC\n",
        "        _, H = \n",
        "            \n",
        "        dst =    \n",
        "        panorama = \n",
        "        ################## Your code ends here #####################\n",
        "                \n",
        "    mask = (panorama[:, :, 0] > 0) | (panorama[:, :, 1] > 0) | (panorama[:, :, 2] > 0)\n",
        "    mask = np.sum(mask, axis = 0)\n",
        "    mask = np.where(mask != 0)\n",
        "\n",
        "    return panorama[:,mask[0][0]:mask[0][-1],:]      \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Ief2SIcxJh"
      },
      "source": [
        "Use the below code to test your implementation. This code just reads in the images, calls the stitch_img() function, and plots the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys3FXtLicxJh"
      },
      "outputs": [],
      "source": [
        "center_img = cv2.imread(\"mountain_center.png\")\n",
        "left_img = cv2.imread(\"mountain_left.png\")\n",
        "right_img = cv2.imread(\"mountain_right.png\")\n",
        "\n",
        "final_img = stitch_img([center_img, left_img, right_img])\n",
        "\n",
        "plt.imshow(cv2.cvtColor(final_img.astype(\"uint8\"), cv2.COLOR_BGR2RGB))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a7638919872a97dffc77c26ea91bcf3162d852293faca833d327e2139c693a5e"
    },
    "kernelspec": {
      "display_name": "cv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "CSE164_Assignment2.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}