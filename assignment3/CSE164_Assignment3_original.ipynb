{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTLtUDfoUAZB"
      },
      "source": [
        "\n",
        "<h1 align=\"center\"> CSE 164 Assignment 3, Spring 2022\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoKWD89YUDrO"
      },
      "source": [
        "\n",
        "<h2 align=\"center\"> 5 Questions, 50 pts, due: 23:59 pm, May 22th, 2022\n",
        "    \n",
        "    Your name:          Student ID: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp9q_ETO7TZk"
      },
      "source": [
        "## Instruction \n",
        "\n",
        "- In this assignment, you will learn how to extract Lucas-Kanade Optical Flow, how to segment a image using mean shift, and how to build a simple neural network.\n",
        "    - **Problem 1: Lucas-Kanade Optical Flow (10 points)**\n",
        "        - Implement the `opticalFlow` function.\n",
        "    - **Problem 2: Centroid Shift (10 points)**\n",
        "        - Implement the `move` function.\n",
        "    - **Problem 3: Mean Shift (10 points)**\n",
        "        - Implement the `mean_shift` function.\n",
        "    - **Problem 5: Build a Neural Network (10 points)**\n",
        "        - Implement the `build_model` function.\n",
        "    - **Problem 5: Train a Neural Network (10 points)**\n",
        "        - Implement the `train_model` function.\n",
        "\n",
        "- Your job is to implement the sections marked with TODO to complete the tasks.\n",
        "\n",
        "- Submit your assignments onto **Canvas** by the due date. Upload a <code>zip</code> file containing:\n",
        "\n",
        "    (1) The saved/latest <code>.ipynb</code> file including the output of all cells.\n",
        "    \n",
        "## Note\n",
        "- This is an **individual** assignment. All help from others (from the web, books other than text, or people other than the TA or instructor) must be clearly acknowledged. \n",
        "- Don't use any magic function from other libraries. You will get **no credit** if a Gaussian function from a known library is used when you are asked to implement “Gaussian Filtering” from scratch.\n",
        "- Don't change the input and output structure of pre-defined functions. Most coding parts can be finished with less than 5 lines of codes.\n",
        "- Make sure you have installed required packages: <code>numpy</code>, <code>matplotlib</code>, <code>PIL</code>, <code>opencv</code>, <code>scipy</code>, <code>scikit-learn</code>, <code>scikit-image</code>, <code>tensorflow</code>, <code>h5py</code>. The code is tested with <code>tensorflow==2.8.0</code> and <code>h5py==3.6.0</code>, but it is possible that this code can run with other versions. \n",
        "\n",
        "## Objective \n",
        "\n",
        "- **Task 1:** Lucas-Kanade Optical Flow\n",
        "- **Task 2:** Mean Shift Image Segmentation\n",
        "- **Task 3:** Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4D33pt5P6WL"
      },
      "source": [
        "Load the Dependencies\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X5MZnp5YHUH"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from scipy.io import loadmat\n",
        "import numpy as np     \n",
        "import h5py                                       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFkAPa3WP9AT"
      },
      "source": [
        "Problem 1: Lucas-Kanade Optical Flow (10 points)\n",
        "-----\n",
        "\n",
        "> Lucas-Kanade method is basically an estimate of the movement of interesting\n",
        "features in successive images of a scene. \n",
        "\n",
        "> Lucas-Kanade assumes that (a) the intensity of the pixel does not change when it moves from frame1 to frame2 (b) small motion (c) motion field and optical flow is constant within a small neighborhood.\n",
        "\n",
        "Implement the **opticalFlow** function below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP8-_5gYWpiE"
      },
      "outputs": [],
      "source": [
        "def inRange(coordinates, limits):\n",
        "    ''' \n",
        "    inRange checks whether the given cordinates line in the given image limits\n",
        "    cordinates, limits are tuples i.e., (X,Y) \n",
        "    '''\n",
        "    x,y = coordinates\n",
        "    X_Limit, Y_Limit = limits\n",
        "    return 0 <= x and x < X_Limit and 0 <= y and y < Y_Limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdrH2tbgUPe4"
      },
      "outputs": [],
      "source": [
        "def opticalFlow(old_frame, new_frame, window_size, min_quality=0.01):\n",
        "    '''\n",
        "    OpticalFlow calculates the displacements in X and Y directions i.e., (u,v)\n",
        "    given two consecutive images varying with time\n",
        "\n",
        "    Input:\n",
        "        old_frame: Image from time T. shape (m,n)\n",
        "        new_frame: Image from time T+1. shape (m,n)\n",
        "        window_size: the window size in which the Lucas-Kanade assumption holds. int\n",
        "        min_quality; controls what features we want to track. int\n",
        "    Output:\n",
        "        dst_img: destination image, shape (m, n, 3)\n",
        "    \n",
        "    TODO: Implement the computation of u,v \n",
        "    '''\n",
        "    max_corners = 10000\n",
        "    min_distance = 0.1\n",
        "    # Corners are better points to be tracked. \n",
        "    # Here we call OpenCV implemented Shi-Tomasi algorithm to find some corner points\n",
        "    feature_list = cv2.goodFeaturesToTrack(old_frame, max_corners, min_quality, min_distance)\n",
        "\n",
        "    w = int(window_size/2)\n",
        "\n",
        "    old_frame = old_frame / 255\n",
        "    new_frame = new_frame / 255\n",
        "\n",
        "    #Convolve to get gradients w.r.to X, Y and T dimensions\n",
        "    kernel_x = np.array([[-1, 1], [-1, 1]])\n",
        "    kernel_y = np.array([[-1, -1], [1, 1]])\n",
        "    kernel_t = np.array([[1, 1], [1, 1]])\n",
        "\n",
        "    fx = cv2.filter2D(old_frame, -1, kernel_x)              #Gradient over X\n",
        "    fy = cv2.filter2D(old_frame, -1, kernel_y)              #Gradient over Y\n",
        "    ft = cv2.filter2D(new_frame, -1, kernel_t) - cv2.filter2D(old_frame, -1, kernel_t)  #Gradient over Time\n",
        "\n",
        "\n",
        "    u = np.zeros(old_frame.shape)\n",
        "    v = np.zeros(old_frame.shape)\n",
        "\n",
        "    for feature in feature_list:        #   for every corner\n",
        "            j, i = feature.ravel()\t\t#   get cordinates of the corners (i,j). They are stored in the order j, i\n",
        "            i, j = int(i), int(j)\t\t#   i,j are floats initially\n",
        "\n",
        "            I_x = fx[i-w:i+w+1, j-w:j+w+1].flatten()\n",
        "            I_y = fy[i-w:i+w+1, j-w:j+w+1].flatten()\n",
        "            I_t = ft[i-w:i+w+1, j-w:j+w+1].flatten()\n",
        "\n",
        "            b = np.reshape(I_t, (I_t.shape[0],1))\n",
        "            A = np.vstack((I_x, I_y)).T\n",
        "\n",
        "            ################## Your code begins here #####################\n",
        "            Pinv_A = \n",
        "            U = # Solving for (u,v) i.e., U here\n",
        "            ################## Your code ends here #####################\n",
        "\n",
        "            u[i,j] = U[0][0]\n",
        "            v[i,j] = U[1][0]\n",
        "\n",
        "    return (u,v)\n",
        "\n",
        "\n",
        "def drawSeperately(old_frame, new_frame, U, V):\n",
        "    '''\n",
        "    Create a plot of the displacement vectors given (u,v) and plot the two images and displacement in a row.\n",
        "    '''\n",
        "    displacement = np.ones_like(img2)            #Fill the displacement plot with White background\n",
        "    displacement.fill(255.)\n",
        "    line_color =  (0, 0, 0)\n",
        "    # draw the displacement vectors\n",
        "    for i in range(img2.shape[0]):\n",
        "        for j in range(img2.shape[1]):\n",
        "            start_pixel = (i,j)\n",
        "            end_pixel = ( int(i+U[i][j]), int(j+V[i][j]) )\n",
        "            #check if there is displacement for the corner and endpoint is in range\n",
        "            if U[i][j] and V[i][j] and inRange( end_pixel, img1.shape ):     \n",
        "                displacement = cv2.arrowedLine( displacement, start_pixel, end_pixel, line_color, thickness =2)\n",
        "\n",
        "    figure, axes = plt.subplots(1,3)\n",
        "    axes[0].imshow(old_frame, cmap = \"gray\")\n",
        "    axes[0].set_title(\"first image\")\n",
        "    axes[1].imshow(new_frame, cmap = \"gray\")\n",
        "    axes[1].set_title(\"second image\")\n",
        "    axes[2].imshow(displacement, cmap = \"gray\")\n",
        "    axes[2].set_title(\"displacements\")\n",
        "    figure.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "ebUGnwpkWnWl",
        "outputId": "80ed6174-3217-4b81-9bd1-82965062f973"
      },
      "outputs": [],
      "source": [
        "img1_url = \"https://raw.githubusercontent.com/Utkal97/Object-Tracking/main/Inputs/basketball1.png\"\n",
        "img2_url = \"https://raw.githubusercontent.com/Utkal97/Object-Tracking/main/Inputs/basketball2.png\"\n",
        "\n",
        "img1 = Image.open(requests.get(img1_url, stream=True).raw)\n",
        "img1 = np.array(img1)\n",
        "print(f'image1 shape: {img1.shape}')\n",
        "img2 = Image.open(requests.get(img2_url, stream=True).raw)\n",
        "img2 = np.array(img2)\n",
        "print(f'image2 shape: {img2.shape}')\n",
        "plt.subplot(121),plt.imshow(img1, cmap=\"gray\"),plt.title('Image1')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.subplot(122),plt.imshow(img2, cmap=\"gray\"),plt.title('Image2')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "OmGFBZSFWnck",
        "outputId": "5068aeff-6f3e-4024-a8e2-91636f772f00"
      },
      "outputs": [],
      "source": [
        "# Obtain (u,v) from Lucas Kanade's optical flow approach\n",
        "U, V = opticalFlow(img1, img2, 3, 0.05)\n",
        "\n",
        "# Save results\n",
        "drawSeperately(img1, img2, U, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZF3z83bpAq3"
      },
      "source": [
        "Mean Shift Setup\n",
        "-----\n",
        "\n",
        "Before we start image segmentation with mean shift aigorithm, first we need to define some global variables and utility functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh0fPX8LpJiv"
      },
      "outputs": [],
      "source": [
        "# global variables\n",
        "MIN_DISTANCE = 5\n",
        "GROUP_DISTANCE = 10\n",
        "COLOR_POOL = [[1,50,200],[150,200,9],[255,0,0],[10,79,0],\\\n",
        "\t\t\t  [200,100,0],[25,38,0],[78,250,250],[158,143,120],[28,222,98],\\\n",
        "\t\t\t  [255,250,199],[100,150,100],[200,50,1],[250,250,78],[15,38,245],\\\n",
        "\t\t\t  [123,234,90]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa1IbGhopWW4"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(x1, x2):\n",
        "\t# weighted on the distance more\n",
        "\tdis = (sum((x1[:2] - x2[:2])**2))*1.5\n",
        "\tcolor = sum((x1[2:] - x2[2:])**2)\n",
        "\treturn np.sqrt(dis + color)\n",
        "\n",
        "# formula: (1/(sigma*sqrt(2*pi)))*e^(-x^2/(2*sigma^2))\n",
        "def gaussian(distance, sigma):\n",
        "\treturn (1/(sigma*math.sqrt(2*math.pi)))*np.exp(-0.5*((distance/sigma))**2)\n",
        " \n",
        "def get_neighbors(all_points, center_point, sigma):\n",
        "\tneighbors = []\n",
        "\tfor p in all_points:\n",
        "\t\tdistance = euclidean_distance(p, center_point)\n",
        "\t\tif distance <= sigma:\n",
        "\t\t\tneighbors.append(p)\n",
        "\treturn neighbors\n",
        "\n",
        "def find_min_dis(group, pos):\n",
        "\tmin_distance = sys.float_info.max\n",
        "\tfor p in group:\n",
        "\t\tmin_distance = min(min_distance, euclidean_distance(p, pos));\n",
        "\treturn min_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os-9zv-TvtED"
      },
      "source": [
        "Problem 2: Centroid Shift (10 points)\n",
        "=========\n",
        "\n",
        "You should finish the **move** function. It receives a center point, points within a certain window of center point, and standard deviation of the gaussian function for weighted averaging. It computes a new centroid by using weighted average of all the points within a centain window.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WRG25CMuQ99"
      },
      "outputs": [],
      "source": [
        "\n",
        "def move(center_point, points, sigma):\n",
        "\t'''\n",
        "    Computes the weighted mean of the density in the window determined by the Gaussian kernel\n",
        "\t(sum of gaussian(distance)*point) / (sum of gaussian(distance))\n",
        "\n",
        "    Input:\n",
        "        center_point: The center of the window. shape(5, ). contains x,y,r,g,b\n",
        "        points: A list of points in the window. each element if of shape(5, ), containing contains x,y,r,g,b\n",
        "        sigma: The standard deviation of gaussian function. float\n",
        "    Output:\n",
        "        new_center_point: The new center of the window. shape(5, ). contains x,y,r,g,b\n",
        "    \n",
        "    TODO: Implement the move function\n",
        "    '''\n",
        "\tsum_x = 0\n",
        "\tsum_y = 0\n",
        "\tsum_r = 0\n",
        "\tsum_g = 0\n",
        "\tsum_b = 0\n",
        "\tsum_weight = 0\n",
        "\n",
        "\tfor p in points:\n",
        "\t\tdistance = euclidean_distance(p, center_point)\n",
        "\t\tweight = gaussian(distance, sigma)\n",
        "        ################## Your code begins here #####################\n",
        "\t\tsum_x +=\n",
        "\t\tsum_y +=\n",
        "\t\tsum_r +=\n",
        "\t\tsum_g +=\n",
        "\t\tsum_b +=\n",
        "\t\tsum_weight +=\n",
        "        ################## Your code ends here #####################\n",
        "\n",
        "\treturn np.array([(sum_x / sum_weight), (sum_y / sum_weight), \\\n",
        "\t\t(sum_r / sum_weight), (sum_g / sum_weight), (sum_b / sum_weight)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT0GQfbuzClM"
      },
      "source": [
        "Group Results\n",
        "-----\n",
        "\n",
        "There is no guarantee that two different points will eventually converge to exactly the same mode. In fact, in practice that barely happens. Thus, two converged centroids that are close enough are classified as belonging to the same cluster. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBhdPwT7uRmE"
      },
      "outputs": [],
      "source": [
        "def group_clusters(positions, points):\n",
        "\t# label each original points\n",
        "\tlabels = [0]\n",
        "\t# a list to group the positions, for the purpose of calculating the min distance\n",
        "\tgrouped_positions = [[positions[0]]]\n",
        "\t# a list to group the original points\n",
        "\t# once we know the lable of the original point, we can quickly find other elements in the same group\n",
        "\tgrouped_points = [np.array([points[0]])]\n",
        "\ttotal_index = 1\n",
        "\tfor i in range(1, len(positions)):\n",
        "\t\tneed_new_index = True\n",
        "\t\tfor index, group in enumerate(grouped_positions):\n",
        "\t\t\tif find_min_dis(group, positions[i]) <= GROUP_DISTANCE:\n",
        "\t\t\t\tlabels.append(index)\n",
        "\t\t\t\tgrouped_positions[index].append(positions[i])\n",
        "\t\t\t\tgrouped_points[index] = np.vstack([grouped_points[index], points[i]])\n",
        "\t\t\t\tneed_new_index = False\n",
        "\t\t\t\tbreak\n",
        "\t\tif need_new_index:\n",
        "\t\t\tlabels.append(total_index)\n",
        "\t\t\tgrouped_positions.append([positions[i]])\n",
        "\t\t\tgrouped_points.append(np.array([points[i]]))\n",
        "\t\t\ttotal_index += 1\n",
        "\n",
        "\treturn labels, np.array(grouped_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3NpHHKCzuKo"
      },
      "source": [
        "Problem 3: Mean Shift (10 points)\n",
        "=========\n",
        "\n",
        "You should finish the **mean_shift** function. It implements the main logic of mean shift algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk2vutkapds4"
      },
      "outputs": [],
      "source": [
        "def mean_shift(points, sigma):\n",
        "\t'''\n",
        "    Mean shift main function. Calls previous functions to finish mean shift segmentation.\n",
        "\n",
        "    Input:\n",
        "        points: shape (m, 5). each row is the concatenation of pixel coordinates x,y and pixel values r,g,b\n",
        "        sigma: The standard deviation of gaussian function. float\n",
        "    Output:\n",
        "        labels: a list of indices of cluster to which each pixel belongs to\n",
        "\t\tgrouped_points: shape (g, n, 5), where g is the number of clusters, \n",
        "\t\tn is the pixel numbers in each cluster, 5 gor x,y,r,g,b\n",
        "    \n",
        "    TODO: Implement the mean_shift function\n",
        "    '''\n",
        "\t# we mark a point as visited when it is converged(measured by distance)\n",
        "\t# type: {tuple: boolean}\n",
        "\tvisited = OrderedDict((tuple(p), False) for p in points)\n",
        "\n",
        "\t# mapping from the original points to points after shifted\n",
        "\t# type: {tuple: ndarray}\n",
        "\toriginal_to_shifted = OrderedDict((tuple(p), p) for p in points)\n",
        "\n",
        "\tconverged = False\n",
        "\titeration = 0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "\n",
        "\t# if not all points have converged\n",
        "\twhile not converged or iteration > 30:\n",
        "\t\tconverged = True\n",
        "\t\tfor original_point in original_to_shifted:\n",
        "\t\t\tcur_pos = original_to_shifted[original_point]\n",
        "\t\t\tif visited[tuple(cur_pos)]:\n",
        "\t\t\t\tcontinue\n",
        "            ################## Your code begins here #####################\n",
        "\t\t\tneighbors = \n",
        "\t\t\tnext_pos = \n",
        "\t\t\toriginal_to_shifted[original_point] = \n",
        "\t\t\t################## Your code ends here #####################\n",
        "\t\t\tdistance_dif = euclidean_distance(cur_pos, next_pos)\n",
        "\t\t\tif distance_dif <= MIN_DISTANCE:\n",
        "\t\t\t\tvisited[tuple(next_pos)] = True\n",
        "\t\t\telse:\n",
        "\t\t\t\tvisited[tuple(next_pos)] = False\n",
        "\t\t\t\tconverged = False\n",
        "\t\titeration += 1\n",
        "\n",
        "\tpositions = list(original_to_shifted.values())\n",
        "\n",
        "\t# after all ponits are converge d, we group and label the clusters\n",
        "\tlabels, grouped_points = group_clusters(positions, points)\n",
        "\n",
        "\treturn labels, grouped_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zON5WcpaIFF"
      },
      "outputs": [],
      "source": [
        "def segmentation(image, points, width, height):\n",
        "    labels, shifted_points = mean_shift(points, 5)\n",
        "\n",
        "    new_image = np.zeros_like(image)\n",
        "    # draw the result of the segmentation\n",
        "    for pts in shifted_points:\n",
        "        for point in pts:\n",
        "            x = point[1]\n",
        "            y = point[0]\n",
        "            new_image[y, x] = np.mean(pts, axis=0).astype(np.uint8)[2:]\n",
        "\n",
        "    new_image_resized = cv2.resize(new_image,(width,height),interpolation=cv2.INTER_CUBIC)\n",
        "    return new_image_resized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "Br8WLAfZ7o4l",
        "outputId": "ec9db725-b3aa-4b2b-e9e2-2d1efbb126fe"
      },
      "outputs": [],
      "source": [
        "url = \"https://news.ucsc.edu/2011/06/images/slugcloseup350.jpg\"  \n",
        "\n",
        "image = io.imread(url)\n",
        "original_height = int(image.shape[0])\n",
        "original_width = int(image.shape[1])\n",
        "print(f'image shape: {image.shape}')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "dVsLv9v-uT9B",
        "outputId": "6b2ee9b5-577b-4381-a630-1c55e9f8f5da"
      },
      "outputs": [],
      "source": [
        "# resize to a smaller size to reduce computational cost\n",
        "# Generally, the larger the resolution, the better the result, and the longer it takes\n",
        "image = cv2.resize(image,(int(original_height/5),int(original_width/5)),interpolation=cv2.INTER_CUBIC)\n",
        "all_points = []\n",
        "\n",
        "for i in range(0, image.shape[0]):\n",
        "  for j in range(0, image.shape[1]):\n",
        "    all_points.append(np.array([i,j] + list(image[i,j]))) # revised\n",
        "\n",
        "segmented_image = segmentation(image, all_points, original_width, original_height)\n",
        "plt.imshow(segmented_image),plt.title('nSegmentation Result')\n",
        "plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural Network Setup\n",
        "-----\n",
        "\n",
        "Before we start building a neural network, first we need to load the dataset and split it into train/val/test set. Here we use Street View House Numbers (SVHN) dataset, which is a digit classification benchmark dataset that contains 600,000 32×32 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_size = 10000\n",
        "# download SVHN_train.hdf5 and SVHN_test.hdf5 from https://drive.google.com/file/d/17Ros6RTqPUN_SCEmJT_eiuLx3OOpbPOU/view?usp=sharing, https://drive.google.com/file/d/1VHMJk0PfJnVsvgCqoV_HhSGD9E7iM8QE/view?usp=sharing\n",
        "with h5py.File('SVHN_train.hdf5', 'r') as f:\n",
        "    shape = f[\"X\"].shape\n",
        "    x_train = f[\"X\"][:shape[0]-val_size]\n",
        "    y_train = f[\"Y\"][:shape[0]-val_size].flatten()\n",
        "    x_val = f[\"X\"][shape[0]-val_size:]\n",
        "    y_val = f[\"Y\"][shape[0] - val_size:].flatten()\n",
        "\n",
        "with h5py.File('SVHN_test.hdf5', 'r') as f:\n",
        "    x_test = f[\"X\"][:]\n",
        "    y_test = f[\"Y\"][:].flatten()\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_val = keras.utils.to_categorical(y_val, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Problem 4: Build a Neural Network (10 points)\n",
        "=========\n",
        "\n",
        "Build your convolutional neural networks by adding some layers. You should use 4 convolution layers and ReLU as the default activation function. The kernel size of both layers should be 3x3. Use 32 and 64 as the number of filters for the first and the second convolutional layers, respectively. Use 128 as the number of filters for two last convolution layers, and 3x3 as the kernel size.After that, flatten your input and add two more dense layers. There should be 1024 units in the first dense with ReLU activation, and use 10 hidden units in the second dense layer with softmax activation. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def build_model():\n",
        "    '''\n",
        "    add multiple layers as instructed to the model.\n",
        "\n",
        "    Output:\n",
        "        a keras neural network model with certain layers.\n",
        "    \n",
        "    TODO: Implement the build_model function\n",
        "    '''\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    ################## Your code begins here #####################\n",
        "    # Add layers\n",
        "    model.add() # call model.add multiplpe times until all necessary layers are added\n",
        "    ################## Your code ends here #####################\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Problem 5: Train a Neural Network (10 points)\n",
        "=========\n",
        "\n",
        "Compile model here and set your initial hyperparameters. Use SGD as the optimizer with initial learning rate 0.01 and the momentum = 0.9. You could choose 'categorical_crossentropy' as your loss function, and the metrics should be 'accuracy'. After that, train your model for 10 epochs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model):\n",
        "    '''\n",
        "    Define the optimizer, compile model, and train the model.\n",
        "    \n",
        "    TODO: Implement the train_model function\n",
        "    '''\n",
        "    ################## Your code ends here #####################\n",
        "    sgd = \n",
        "    # compile before start training. you need to specify parameters.\n",
        "    model.compile()\n",
        "    # use model.fit for training. you need to specify parameters.\n",
        "    model.fit()\n",
        "    ################## Your code ends here #####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "train_model(model)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(test_loss, test_acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CSE164_Assignment3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
